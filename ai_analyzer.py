#!/usr/bin/env python3
"""
AI Analyzer - –£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ LLM
–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: github.com/coderroleggg/autoeditor

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ "bad takes" (–Ω–µ—É–¥–∞—á–Ω—ã—Ö –¥—É–±–ª–µ–π)
2. –ü–æ–∏—Å–∫ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∏ –æ—à–∏–±–æ–∫
3. –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–æ —Å—Ü–µ–Ω–∞—Ä–∏–µ–º
4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã—Ä–µ–∑–∫–∞–º
"""

import os
import json
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class Take:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –¥—É–±–ª—è"""
    start: float
    end: float
    text: str
    confidence: float
    is_good: bool
    reason: str = ""


class AIAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI API –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:
    - Bad takes (–ø–æ–≤—Ç–æ—Ä—ã, –æ–≥–æ–≤–æ—Ä–∫–∏)
    - Best takes (—Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏)
    - –ù–µ–Ω—É–∂–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    """

    def __init__(self, api_key: Optional[str] = None, use_local_llm: bool = False):
        """
        Args:
            api_key: OpenAI API –∫–ª—é—á (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è)
            use_local_llm: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é LLM (Ollama/LM Studio)
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.use_local_llm = use_local_llm

    def analyze_transcription(
        self,
        transcription: List[Dict],
        script: Optional[str] = None
    ) -> List[Take]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç good/bad takes

        Args:
            transcription: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ —Ç–∞–π–º–∫–æ–¥–∞–º–∏
            script: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è

        Returns:
            –°–ø–∏—Å–æ–∫ Take –æ–±—ä–µ–∫—Ç–æ–≤ —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π good/bad
        """
        takes = []

        # –ü—Ä–æ—Å—Ç–æ–π —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–±–µ–∑ API)
        takes = self._heuristic_analysis(transcription)

        # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω API - –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if self.api_key and not self.use_local_llm:
            takes = self._llm_analysis(transcription, script, takes)
        elif self.use_local_llm:
            takes = self._local_llm_analysis(transcription, script, takes)

        return takes

    def _heuristic_analysis(self, transcription: List[Dict]) -> List[Take]:
        """
        –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ LLM
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç bad takes –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º:
        - –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        - –§–∏–ª–ª–µ—Ä—ã ("—ç–º", "–∞–∞", "—ç—Ç–æ —Å–∞–º–æ–µ")
        - –î–ª–∏–Ω–Ω—ã–µ –ø–∞—É–∑—ã –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        takes = []

        # –ú–∞—Ä–∫–µ—Ä—ã bad takes
        filler_words = ['—ç–º', '—ç—ç', '–∞–∞', '—ç—Ç–æ —Å–∞–º–æ–µ', '–∫–∞–∫ –±—ã', '–Ω—É']

        for i, segment in enumerate(transcription):
            text = segment['text'].lower().strip()
            start = segment['start']
            end = segment['end']

            is_good = True
            reason = "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç"
            confidence = 1.0

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–∏–ª–ª–µ—Ä—ã
            filler_count = sum(1 for filler in filler_words if filler in text)
            if filler_count >= 2:
                is_good = False
                reason = f"–ú–Ω–æ–≥–æ —Ñ–∏–ª–ª–µ—Ä–æ–≤ ({filler_count})"
                confidence = 0.3

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–µ–≥–º–µ–Ω—Ç–æ–º
            if i > 0:
                prev_text = transcription[i-1]['text'].lower().strip()
                # –ü—Ä–æ—Å—Ç–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤
                prev_words = set(prev_text.split())
                curr_words = set(text.split())
                overlap = len(prev_words & curr_words) / max(len(curr_words), 1)

                if overlap > 0.7 and len(text.split()) > 3:
                    is_good = False
                    reason = "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"
                    confidence = 0.2

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã (–æ–±—Ä—ã–≤–∫–∏)
            if len(text.split()) < 3 and end - start < 1.0:
                is_good = False
                reason = "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç"
                confidence = 0.4

            takes.append(Take(
                start=start,
                end=end,
                text=segment['text'],
                confidence=confidence,
                is_good=is_good,
                reason=reason
            ))

        return takes

    def _llm_analysis(
        self,
        transcription: List[Dict],
        script: Optional[str],
        heuristic_takes: List[Take]
    ) -> List[Take]:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ OpenAI API

        –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω openai:
        pip install openai
        """
        try:
            import openai

            client = openai.OpenAI(api_key=self.api_key)

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
            transcript_text = "\n".join([
                f"[{seg['start']:.1f}s - {seg['end']:.1f}s] {seg['text']}"
                for seg in transcription
            ])

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç
            script_section = f"–°—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è:\n{script}\n\n" if script else ""

            prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –≤–∏–¥–µ–æ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ good/bad takes.

Bad takes - —ç—Ç–æ:
- –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ
- –û–≥–æ–≤–æ—Ä–∫–∏ –∏ –ø–µ—Ä–µ–¥–µ–ª—ã–≤–∞–Ω–∏—è —Ñ—Ä–∞–∑—ã
- –§–∏–ª–ª–µ—Ä—ã ("—ç–º", "–∞–∞", "—ç—Ç–æ —Å–∞–º–æ–µ")
- –ù–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –º—ã—Å–ª–∏ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π

Good takes - —ç—Ç–æ:
- –§–∏–Ω–∞–ª—å–Ω—ã–µ —É–¥–∞—á–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
- –ß–∏—Å—Ç–∞—è —Ä–µ—á—å –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤
- –ó–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –º—ã—Å–ª–∏

–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:
{transcript_text}

{script_section}–í–µ—Ä–Ω–∏ JSON –º–∞—Å—Å–∏–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞:
[
  {{"start": 0.0, "end": 5.0, "is_good": true, "reason": "–ß–∏—Å—Ç–∞—è —Ä–µ—á—å", "confidence": 0.9}},
  ...
]
"""

            response = client.chat.completions.create(
                model="gpt-4o-mini",  # –ò–ª–∏ gpt-4o –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤–∏–¥–µ–æ–º–æ–Ω—Ç–∞–∂—É. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—à—å good/bad takes."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.3
            )

            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            analysis = json.loads(response.choices[0].message.content)

            # –û–±–Ω–æ–≤–ª—è–µ–º takes –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –∞–Ω–∞–ª–∏–∑–∞
            if 'segments' in analysis:
                for i, llm_take in enumerate(analysis['segments']):
                    if i < len(heuristic_takes):
                        heuristic_takes[i].is_good = llm_take.get('is_good', True)
                        heuristic_takes[i].reason = llm_take.get('reason', 'LLM –∞–Ω–∞–ª–∏–∑')
                        heuristic_takes[i].confidence = llm_take.get('confidence', 0.8)

            return heuristic_takes

        except ImportError:
            print("‚ö†Ô∏è  OpenAI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑.")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            return heuristic_takes
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return heuristic_takes

    def _local_llm_analysis(
        self,
        transcription: List[Dict],
        script: Optional[str],
        heuristic_takes: List[Take]
    ) -> List[Take]:
        """
        –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é LLM (Ollama, LM Studio)

        –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏:
        - Ollama: https://ollama.ai
        - –ú–æ–¥–µ–ª—å: ollama pull llama3
        """
        try:
            import requests

            transcript_text = "\n".join([
                f"[{seg['start']:.1f}s] {seg['text']}"
                for seg in transcription
            ])

            prompt = f"""–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –≤–∏–¥–µ–æ. –û–ø—Ä–µ–¥–µ–ª–∏ good/bad takes.

Bad: –ø–æ–≤—Ç–æ—Ä—ã, –æ–≥–æ–≤–æ—Ä–∫–∏, —Ñ–∏–ª–ª–µ—Ä—ã
Good: —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —á–∏—Å—Ç—ã–µ –≤–µ—Ä—Å–∏–∏

{transcript_text}

JSON –æ—Ç–≤–µ—Ç —Å is_good, reason –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞:"""

            # –ó–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': 'llama3',
                    'prompt': prompt,
                    'stream': False,
                    'format': 'json'
                }
            )

            if response.status_code == 200:
                result = response.json()
                analysis = json.loads(result['response'])

                # –û–±–Ω–æ–≤–ª—è–µ–º takes
                # (–ª–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞ _llm_analysis)

            return heuristic_takes

        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM: {e}")
            return heuristic_takes

    def generate_recommendations(self, takes: List[Take]) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–Ω—Ç–∞–∂—É

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏:
            - segments_to_remove: –°–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            - segments_to_keep: –°–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            - statistics: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        bad_takes = [t for t in takes if not t.is_good]
        good_takes = [t for t in takes if t.is_good]

        total_bad_duration = sum(t.end - t.start for t in bad_takes)
        total_good_duration = sum(t.end - t.start for t in good_takes)

        return {
            'segments_to_remove': [
                {
                    'start': t.start,
                    'end': t.end,
                    'reason': t.reason,
                    'text': t.text[:50] + '...' if len(t.text) > 50 else t.text
                }
                for t in bad_takes
            ],
            'segments_to_keep': [
                {'start': t.start, 'end': t.end}
                for t in good_takes
            ],
            'statistics': {
                'total_segments': len(takes),
                'bad_takes': len(bad_takes),
                'good_takes': len(good_takes),
                'bad_duration': total_bad_duration,
                'good_duration': total_good_duration,
                'time_saved': total_bad_duration
            }
        }

    def export_analysis(self, takes: List[Take], output_path: str):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤ JSON"""
        data = {
            'takes': [
                {
                    'start': t.start,
                    'end': t.end,
                    'text': t.text,
                    'is_good': t.is_good,
                    'confidence': t.confidence,
                    'reason': t.reason
                }
                for t in takes
            ]
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == '__main__':
    # –¢–µ—Å—Ç–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
    test_transcription = [
        {'start': 0.0, 'end': 3.0, 'text': '–ü—Ä–∏–≤–µ—Ç, —ç—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –≤–∏–¥–µ–æ'},
        {'start': 3.5, 'end': 6.0, 'text': '—ç–º, —ç—Ç–æ, –∫–∞–∫ –±—ã, —Ç–µ—Å—Ç–æ–≤–æ–µ –≤–∏–¥–µ–æ'},
        {'start': 6.5, 'end': 9.0, 'text': '–ü—Ä–∏–≤–µ—Ç, —ç—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –≤–∏–¥–µ–æ'},
        {'start': 9.5, 'end': 12.0, 'text': '–°–µ–≥–æ–¥–Ω—è –º—ã –ø–æ–≥–æ–≤–æ—Ä–∏–º –æ –º–æ–Ω—Ç–∞–∂–µ'},
    ]

    analyzer = AIAnalyzer()
    takes = analyzer.analyze_transcription(test_transcription)

    print("\nüìä –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–µ–π:")
    print("=" * 60)
    for take in takes:
        status = "‚úÖ GOOD" if take.is_good else "‚ùå BAD"
        print(f"\n{status} [{take.start:.1f}s - {take.end:.1f}s]")
        print(f"   –¢–µ–∫—Å—Ç: {take.text}")
        print(f"   –ü—Ä–∏—á–∏–Ω–∞: {take.reason}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {take.confidence:.1%}")

    recommendations = analyzer.generate_recommendations(takes)
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {recommendations['statistics']['total_segments']}")
    print(f"   Good takes: {recommendations['statistics']['good_takes']}")
    print(f"   Bad takes: {recommendations['statistics']['bad_takes']}")
    print(f"   –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏: {recommendations['statistics']['time_saved']:.1f}s")
