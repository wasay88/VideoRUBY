#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–±—Ç–∏—Ç—Ä–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI Whisper –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏
"""

import os
import subprocess
import json
from pathlib import Path
from typing import List, Dict


class Transcriber:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç"""

    def __init__(self, model_size: str = "base", language: str = "ru"):
        """
        Args:
            model_size: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ Whisper (tiny, base, small, medium, large)
            language: –ö–æ–¥ —è–∑—ã–∫–∞ (ru –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
        """
        self.model_size = model_size
        self.language = language

    def check_whisper_installed(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ Whisper"""
        try:
            result = subprocess.run(
                ['whisper', '--help'],
                capture_output=True,
                text=True
            )
            return result.returncode == 0
        except FileNotFoundError:
            return False

    def transcribe(self, video_path: str, output_format: str = "srt") -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å—É–±—Ç–∏—Ç—Ä—ã

        Args:
            video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª—É
            output_format: –§–æ—Ä–º–∞—Ç —Å—É–±—Ç–∏—Ç—Ä–æ–≤ (srt, vtt, txt, json)

        Returns:
            –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏
        """
        print(f"üé§ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é –∞—É–¥–∏–æ (–º–æ–¥–µ–ª—å: {self.model_size}, —è–∑—ã–∫: {self.language})...")

        if not self.check_whisper_installed():
            raise RuntimeError(
                "Whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∫–æ–º–∞–Ω–¥–æ–π:\n"
                "pip install openai-whisper"
            )

        video_path = os.path.abspath(video_path)
        output_dir = os.path.dirname(video_path)

        # –ó–∞–ø—É—Å–∫–∞–µ–º Whisper
        cmd = [
            'whisper',
            video_path,
            '--model', self.model_size,
            '--language', self.language,
            '--output_format', output_format,
            '--output_dir', output_dir
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {result.stderr}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É —Å—É–±—Ç–∏—Ç—Ä–æ–≤
        base_name = Path(video_path).stem
        subtitle_path = os.path.join(output_dir, f"{base_name}.{output_format}")

        if os.path.exists(subtitle_path):
            print(f"‚úÖ –°—É–±—Ç–∏—Ç—Ä—ã —Å–æ–∑–¥–∞–Ω—ã: {subtitle_path}")
            return subtitle_path
        else:
            raise FileNotFoundError(f"–§–∞–π–ª —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {subtitle_path}")

    def transcribe_segments(self, video_path: str, segments: List) -> List[Dict]:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –≤–∏–¥–µ–æ

        Args:
            video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ
            segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–∏–∑ video_processor)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        subtitle_path = self.transcribe(video_path, output_format="json")

        # –ß–∏—Ç–∞–µ–º JSON —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
        with open(subtitle_path, 'r', encoding='utf-8') as f:
            transcription_data = json.load(f)

        # Whisper JSON —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–µ–≥–º–µ–Ω—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        whisper_segments = transcription_data.get('segments', [])

        subtitles = []
        for seg in whisper_segments:
            subtitles.append({
                'start': seg['start'],
                'end': seg['end'],
                'text': seg['text'].strip()
            })

        return subtitles


class SubtitleGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""

    @staticmethod
    def format_time_srt(seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –¥–ª—è SRT —Ñ–æ—Ä–º–∞—Ç–∞"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

    @staticmethod
    def generate_srt(subtitles: List[Dict], output_path: str):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç SRT —Ñ–∞–π–ª

        Args:
            subtitles: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ start, end, text
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è SRT —Ñ–∞–π–ª–∞
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            for i, sub in enumerate(subtitles, 1):
                start_time = SubtitleGenerator.format_time_srt(sub['start'])
                end_time = SubtitleGenerator.format_time_srt(sub['end'])

                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{sub['text']}\n")
                f.write("\n")

        print(f"‚úÖ SRT —Å—É–±—Ç–∏—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")

    @staticmethod
    def adjust_subtitles_for_edited_video(
        subtitles: List[Dict],
        segments: List,
        output_path: str
    ):
        """
        –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –¥–ª—è –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ
        (–ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø–∞—É–∑)

        Args:
            subtitles: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã
            segments: –°–µ–≥–º–µ–Ω—Ç—ã –∏–∑ video_processor
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—É–±—Ç–∏—Ç—Ä–æ–≤
        """
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —Å—Ç–∞—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –Ω–æ–≤–æ–µ
        speech_segments = [s for s in segments if s.is_speech]

        adjusted_subtitles = []
        accumulated_time = 0

        for sub in subtitles:
            sub_start = sub['start']
            sub_end = sub['end']

            # –ù–∞—Ö–æ–¥–∏–º –≤ –∫–∞–∫–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —ç—Ç–æ—Ç —Å—É–±—Ç–∏—Ç—Ä
            for seg in speech_segments:
                if seg.start <= sub_start <= seg.end:
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å–º–µ—â–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞
                    offset_in_segment = sub_start - seg.start
                    new_start = accumulated_time + offset_in_segment

                    # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤–æ–µ –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
                    duration = sub_end - sub_start
                    new_end = new_start + duration

                    adjusted_subtitles.append({
                        'start': new_start,
                        'end': new_end,
                        'text': sub['text']
                    })
                    break

            # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            for seg in speech_segments:
                if seg.end <= sub_start:
                    accumulated_time = seg.end - seg.start

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã
        SubtitleGenerator.generate_srt(adjusted_subtitles, output_path)

        return adjusted_subtitles


if __name__ == '__main__':
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
    import sys

    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python transcription.py <–ø—É—Ç—å_–∫_–≤–∏–¥–µ–æ>")
        sys.exit(1)

    video_path = sys.argv[1]
    transcriber = Transcriber(model_size="base", language="ru")

    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
    subtitle_path = transcriber.transcribe(video_path, output_format="srt")
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°—É–±—Ç–∏—Ç—Ä—ã: {subtitle_path}")
